# -*- coding: utf-8 -*-
"""neural_net_training_10th_1_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M9M2GKdPvNFxY4enwgF4pb-B1Mg5cD8I

# Importing necessary libraries
"""

import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.metrics import f1_score
from transformers import TFBertForSequenceClassification, BertTokenizer, create_optimizer
from tensorflow.keras import regularizers
from tensorflow.keras.layers import Dropout
from tqdm import tqdm

"""# Loading the dataset"""

from google.colab import drive
drive.mount('/content/drive')

train_df = pd.read_csv("/content/drive/MyDrive/train.csv")
test_df = pd.read_csv("/content/drive/MyDrive/dev.csv")

"""# Split into text and target columns"""

text_column='text'
target_columns=['admiration','amusement','gratitude','love','pride','relief','remorse']

from sklearn.model_selection import train_test_split

# Split train data into texts and labels
train_texts, train_labels = train_df[text_column].tolist(), train_df[target_columns].values

# Split the train dataset into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts, train_labels, test_size=0.2, random_state=42
)

# Extract texts and labels for the test dataset
test_texts, test_labels = test_df[text_column].tolist(), test_df[target_columns].values

"""# Load the Tokenizer"""

# Load tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

"""# Text Encoding"""

#Encode takes raw text data and converts it into numerical token IDs
def encode_texts(texts, tokenizer, max_length=256):  # Increase to 256 or 512
    return tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="tf"
    )

# Tokenize and encode the texts for training, validation, and testing
train_encodings = encode_texts(train_texts, tokenizer)
val_encodings=encode_texts(val_texts,tokenizer)
test_encodings = encode_texts(test_texts, tokenizer)

# Training dataset
# Create TensorFlow datasets for efficient data loading during training

train_dataset = tf.data.Dataset.from_tensor_slices((
    {"input_ids": train_encodings["input_ids"], "attention_mask": train_encodings["attention_mask"]},
    train_labels
)).batch(16).prefetch(tf.data.AUTOTUNE)

# Validation dataset
val_dataset = tf.data.Dataset.from_tensor_slices((
    {"input_ids": val_encodings["input_ids"], "attention_mask": val_encodings["attention_mask"]},
    val_labels
)).batch(16).prefetch(tf.data.AUTOTUNE)

# Test dataset
test_dataset = tf.data.Dataset.from_tensor_slices((
    {"input_ids": test_encodings["input_ids"], "attention_mask": test_encodings["attention_mask"]},
    test_labels
)).batch(16).prefetch(tf.data.AUTOTUNE)

"""# Creating and Initializing the Model"""

def create_model_with_regularization_and_dropout(emotion_columns, learning_rate, weight_decay, dropout_rate):
    """
    Create a TFBertForSequenceClassification model with L2 regularization and Dropout.
    """
    # Load the pre-trained BERT model
    model = TFBertForSequenceClassification.from_pretrained(
        "bert-base-uncased",
        num_labels=len(emotion_columns)
    )

    # Apply L2 regularization to the classifier layer
    model.classifier.kernel_regularizer = regularizers.L2(weight_decay)

    # Adjust Dropout layer
    model.dropout = Dropout(dropout_rate)

    # Define optimizer-This enables a learning rate scheduler where it starts small and gradually decays over the remaining training steps
    num_train_steps = len(train_dataset) * 5  # Total training steps for 5 epochs
    optimizer, _ = create_optimizer(
        init_lr=learning_rate,
        num_warmup_steps=0.1 * num_train_steps,  # 10% warm-up
        num_train_steps=num_train_steps,
        weight_decay_rate=weight_decay  #Since large pre-trained BERT models are prone to overfitting,specifically on small or imbalanced datasets
    )

    # Compile the model
    model.compile(
        optimizer=optimizer,
        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
        metrics=["accuracy"]
    )
    return model, optimizer

# Instantiate the model and optimizer
model, optimizer = create_model_with_regularization_and_dropout(
    emotion_columns=target_columns,
    learning_rate=2e-5,
    weight_decay=0.01,
    dropout_rate=0.3
)

"""# Custom Training Loop"""

def custom_training_loop(model, train_dataset, val_dataset, optimizer, loss_fn, epochs, patience, factor, min_lr):
    """
    Custom training loop with early stopping and learning rate scheduler.
    """
    #Initialize tracking variables for early stopping

    best_val_loss = float("inf")  # Initialize with infinity to ensure first epoch is always "better"
    wait = 0  # Patience counter
    best_weights = None # Store the best model weights

    for epoch in range(epochs):
        print(f"\nEpoch {epoch + 1}/{epochs}")

        # Training
        print("Training...")
        train_loss = 0
        for step, (inputs, labels) in tqdm(enumerate(train_dataset), total=len(train_dataset)):
            with tf.GradientTape() as tape:   # Use gradient tape for automatic differentiation
                logits = model(inputs, training=True).logits
                loss = loss_fn(labels, logits)
            gradients = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, model.trainable_variables))
            train_loss += loss.numpy()

        train_loss /= len(train_dataset)

        # Validation
        print("Validating...")
        val_loss = 0
        val_preds = []
        val_labels = []
        for step, (inputs, labels) in tqdm(enumerate(val_dataset), total=len(val_dataset)):
            logits = model(inputs, training=False).logits
            loss = loss_fn(labels, logits)
            val_loss += loss.numpy()

            # Collect predictions for F1 score
            probs = tf.sigmoid(logits).numpy()
            preds = (probs > 0.5).astype(int)
            val_preds.extend(preds)
            val_labels.extend(labels.numpy())

        val_loss /= len(val_dataset)
        val_preds = np.array(val_preds)
        val_labels = np.array(val_labels)

        # Calculate F1 score for validation set
        val_f1 = f1_score(val_labels, val_preds, average="micro")
        print(f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}")

        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_weights = model.get_weights()
            wait = 0
        else:
            wait += 1
            if wait >= patience:
                print("Early stopping triggered. Restoring best weights.")
                model.set_weights(best_weights)
                break

        # Learning rate reduction
        if wait >= patience:
            new_lr = max(optimizer.learning_rate.numpy() * factor, min_lr)
            print(f"Reducing learning rate to {new_lr}")
            optimizer.learning_rate.assign(new_lr)
            wait = 0

    return model

"""#Training the model"""

# Train the model
trained_model = custom_training_loop(
    model=model,
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    optimizer=optimizer,
    loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    epochs=5,
    patience=3,  # Early stopping patience
    factor=0.5,  # Learning rate reduction factor
    min_lr=1e-6  # Minimum learning rate
)

"""# Saving the trained model"""

# Define the path where you want to save the model
model_path = "/content/drive/MyDrive/final_model"

# Define the path to save the model in the recommended format
keras_model_path = model_path

# Save the model in TensorFlow's native format
trained_model.save(keras_model_path)

# Confirm the model was saved
print(f"Model saved in the recommended format at: {keras_model_path}")

"""#Saving the weights"""

# Define the path to save weights
weights_path = "/content/drive/MyDrive/models/"

# Save only the weights
trained_model.save_weights(weights_path)

# Confirm the weights were saved
print(f"Model weights saved at: {weights_path}")

"""#Load the trained model"""

# Load weights into the model
trained_model.load_weights(weights_path)

"""#Predictions on Testset"""

# Make predictions with loaded model
def predict_with_loaded_model(model, test_dataset):
    """
    Make predictions using the loaded model
    """
    predictions = []

    for batch in test_dataset:
        inputs = batch[0]  # Assuming batch is (inputs, labels)
        outputs = model(inputs, training=False)
        probs = tf.sigmoid(outputs.logits)
        predictions.extend(probs.numpy())

    return np.array(predictions)

predictions = predict_with_loaded_model(trained_model, test_dataset)

"""#Calculate F1 score"""

def calculate_f1_score(predictions, test_dataset, threshold=0.5):
    """
    Calculate F1 score from predictions and true labels.
    """
    # Convert probabilities to binary predictions
    binary_predictions = (predictions > threshold).astype(int)

    # Extract true labels from the test dataset
    true_labels = []
    for batch in test_dataset:
        labels = batch[1]  # Assuming batch is (inputs, labels)
        true_labels.extend(labels.numpy())

    true_labels = np.array(true_labels)

    # Calculate F1 score
    f1 = f1_score(true_labels, binary_predictions, average="micro")
    return f1

f1 = calculate_f1_score(predictions, test_dataset)
print(f"Micro F1 Score: {f1:.4f}")

"""#Save Predictions"""

def save_predictions_to_csv(predictions, texts, emotion_columns, output_csv_path, threshold=0.5):
    """
    Save predictions as a CSV file with text and binary predictions for each emotion.

    """
    # Convert probabilities to binary predictions
    binary_predictions = (predictions > threshold).astype(int)

    # Create a DataFrame with text and predictions
    results_df = pd.DataFrame({
        "text": texts
    })
    for i, emotion in enumerate(emotion_columns):
        results_df[emotion] = binary_predictions[:, i]

    # Save DataFrame to CSV
    results_df.to_csv(output_csv_path, index=False)
    print(f"Predictions saved to {output_csv_path}")

save_predictions_to_csv(
    predictions=predictions,  # Replace with actual predictions array
    texts=test_df['text'],  # Replace with actual test texts
    emotion_columns=target_columns,
    output_csv_path="predictions_final_dev.csv"
)

"""#Running the model on the test set"""

final_test_df=pd.read_csv('/content/drive/MyDrive/test-in.csv')

# Extract texts and labels for the test dataset
final_test_texts, final_test_labels = final_test_df[text_column].tolist(), final_test_df[target_columns].values

final_test_encodings = encode_texts(final_test_texts, tokenizer)

final_test_dataset = tf.data.Dataset.from_tensor_slices((
    {"input_ids": final_test_encodings["input_ids"], "attention_mask": final_test_encodings["attention_mask"]},
    final_test_labels
)).batch(16).prefetch(tf.data.AUTOTUNE)

predictions_final_test = predict_with_loaded_model(trained_model, final_test_dataset)

f1 = calculate_f1_score(predictions_final_test, final_test_dataset)
print(f"Micro F1 Score: {f1:.4f}")

save_predictions_to_csv(
    predictions=predictions_final_test,  # Replace with actual predictions array
    texts=final_test_df['text'],  # Replace with actual test texts
    emotion_columns=target_columns,
    output_csv_path="predictions_final_test_3.csv"
)

